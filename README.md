# A Replication Study of Swish Activation Function

## Introduction
In this work, we reproduce the paper "Searching for Activation Functions". Specifically, we compare the performance of the novel activation function SWISH to two of the most used conventional activation functions; ReLU and Leaky ReLU.

## Scope of Reproducibility
The original paper proposes that novel activation functions acquired by search methods can perform with higher accuracy than conventional ones. We aim to validate this claim.

## Methodology
We fine-tuned a community-written code and performed some of the paperâ€™s experiments on the same datasets they were performed on.

## Results
The results show that the models performed with higher accuracy when implemented with the SWISH activation function, confirming the claim of the original paper.

## Conclusion
This study confirms that the SWISH activation function outperforms traditional activation functions like ReLU and Leaky ReLU in certain scenarios.

## Authors
- Hussein Hasan - [hussein.hasan@bahcesehir.edu.tr](mailto:hussein.hasan@bahcesehir.edu.tr)
- Majd Owda - [majd.owda@bahcesehir.edu.tr](mailto:majd.owda@bahcesehir.edu.tr)
