# Reproducibility-Swish-Activation
This  project aims to reproduce and validate the findings of the original paper titled "Searching for Activation Functions". In the original study, the authors introduced a novel activation function called SWISH, which was found to outperform traditional activation functions like ReLU and Leaky ReLU in certain deep learning tasks.
